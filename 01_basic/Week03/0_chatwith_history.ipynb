{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to The ChatGPT APIs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install or update the OpenAI Python library first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook cleaned.\n"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "import sys\n",
    "\n",
    "def clean_notebook():\n",
    "    IPython.display.clear_output(wait=True)\n",
    "    print(\"Notebook cleaned.\")\n",
    "\n",
    "!pip install openai\n",
    "!pip install gradio\n",
    "\n",
    "# Clean up the notebook\n",
    "clean_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile .env\n",
    "# OPENAI_API_KEY=your_api_key_here\n",
    "# OPENTYPHOON_API_KEY=api_key_here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "model_name = \"gpt-4.1\"    \n",
    "\n",
    "# openai_client = OpenAI(api_key=os.environ.get(\"OPENTYPHOON_API_KEY\"),base_url=\"https://api.opentyphoon.ai/v1\")\n",
    "# model_name = \"typhoon-instruct\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load environment variables, you can use anything you like but I used `python-dotenv`. Just create a `.env` file with your `OPENAI_API_KEY` then load it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic ChatGPT API Call"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a basic chat API call to learn about the chat format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate the completion\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI research assistant. You use a tone that is technical and scientific.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello, who are you?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Greetings! I am an AI research assistant. How can I help you today?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you tell me about the creation of black holes?\"}\n",
    "    ],\n",
    "    temperature=0.8,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly. Black holes are regions of spacetime exhibiting gravitational fields so intense that nothing—not even light—can escape from within their event horizons. The creation of black holes is understood primarily through the framework of general relativity and astrophysics.\n",
       "\n",
       "**Stellar-Mass Black Holes:**\n",
       "Most commonly, black holes form from the gravitational collapse of massive stars (typically those with initial masses > 20–25 solar masses). At the end of their life cycles, such stars undergo supernova explosions after exhausting their nuclear fuel. If the remnant core's mass exceeds the Tolman–Oppenheimer–Volkoff (TOV) limit (about 2–3 solar masses), neutron degeneracy pressure cannot halt collapse, and the core contracts into a singularity surrounded by an event horizon—a black hole.\n",
       "\n",
       "**Supermassive Black Holes:**\n",
       "These are found at the centers of most galaxies, with masses ranging from millions to billions of solar masses. Their formation remains an open question, but leading hypotheses include the direct collapse of massive gas clouds in the early universe, mergers of many smaller black holes, or rapid accretion processes.\n",
       "\n",
       "**Primordial Black Holes:**\n",
       "A speculative class, these could have formed in the early universe from density fluctuations soon after the Big Bang, long before stars existed. They remain hypothetical, but are of interest in studies of dark matter and early-universe cosmology.\n",
       "\n",
       "**Other Formation Channels:**\n",
       "- **Black Hole Mergers:** The coalescence of two black holes (detected via gravitational waves, e.g., by LIGO/Virgo) results in a more massive black hole.\n",
       "- **Accretion-Induced Collapse:** Neutron stars in binary systems that accrete enough mass may collapse into black holes.\n",
       "- **Runaway Stellar Mergers:** In dense star clusters, repeated stellar collisions can produce sufficiently massive stars that collapse into intermediate-mass black holes.\n",
       "\n",
       "**Key Theoretical Concepts:**\n",
       "- **Event Horizon:** The boundary beyond which escape velocity exceeds the speed of light.\n",
       "- **Singularity:** The point (or region) of infinite density at black hole centers (as predicted by classical general relativity).\n",
       "- **No-Hair Theorem:** Black holes are characterized externally only by mass, charge, and angular momentum.\n",
       "\n",
       "If you would like details on a specific black hole formation scenario or the relevant physics, please specify!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "import sys\n",
    "# pretty format the response\n",
    "\n",
    "IPython.display.Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "model_name = \"gpt-4.1\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the OpenAI client library\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def get_chat_completion(messages):\n",
    "    \"\"\"\n",
    "    Sends a list of messages to the OpenAI chat completions API and returns the response.\n",
    "\n",
    "    Args:\n",
    "        messages (list): A list of message dictionaries, each with 'role' and 'content' keys.\n",
    "                         Example: [{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the assistant's reply.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model_name, # You can choose other models like \"gpt-4\"\n",
    "            messages=messages\n",
    "        )\n",
    "        # Extract the assistant's reply from the response\n",
    "        assistant_reply = response.choices[0].message.content\n",
    "        return assistant_reply\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"Error: Could not get a response from the model.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Multi-Round Conversation Example ---\n",
    "\n",
    "# 1. Initialize the conversation history\n",
    "# This list will store all messages (user and assistant) in the conversation.\n",
    "conversation_history = []\n",
    "\n",
    "conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What's the highest mountain in the world?\n",
      "Assistant: The highest mountain in the world is **Mount Everest**, which stands at 8,848.86 meters (29,031.7 feet) above sea level. It is located in the Himalayas, on the border between Nepal and the Tibet Autonomous Region of China.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# First round of conversation\n",
    "user_message_1 = {\"role\": \"user\", \"content\": \"What's the highest mountain in the world?\"}\n",
    "conversation_history.append(user_message_1)\n",
    "\n",
    "print(f\"User: {user_message_1['content']}\")\n",
    "assistant_response_1 = get_chat_completion(conversation_history)\n",
    "print(f\"Assistant: {assistant_response_1}\")\n",
    "\n",
    "\n",
    "# Add the assistant's response to the conversation history\n",
    "conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response_1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': \"What's the highest mountain in the world?\"},\n",
       " {'role': 'assistant',\n",
       "  'content': 'The highest mountain in the world is **Mount Everest**, which stands at 8,848.86 meters (29,031.7 feet) above sea level. It is located in the Himalayas, on the border between Nepal and the Tibet Autonomous Region of China.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is the second?\n",
      "Assistant: The second highest mountain in the world is **K2** (also known as Mount Godwin-Austen or Chhogori). K2 has an elevation of **8,611 meters** (28,251 feet) above sea level and is located on the border between Pakistan and China, in the Karakoram range.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Second round of conversation\n",
    "user_message_2 = {\"role\": \"user\", \"content\": \"What is the second?\"}\n",
    "conversation_history.append(user_message_2)\n",
    "\n",
    "print(f\"User: {user_message_2['content']}\")\n",
    "assistant_response_2 = get_chat_completion(conversation_history)\n",
    "print(f\"Assistant: {assistant_response_2}\")\n",
    "\n",
    "# Add the assistant's response to the conversation history\n",
    "conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response_2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What's the highest mountain in the world?\n",
      "Assistant: The highest mountain in the world is Mount Everest, which stands at 8,848.86 meters (29,031.7 feet) above sea level. It is located in the Himalayas, on the border between Nepal and the Tibet Autonomous Region of China.\n",
      "User: What is the second?\n",
      "Assistant: The second highest mountain in the world is K2, also known as Mount Godwin-Austen. It stands at 8,611 meters (28,251 feet) above sea level and is located in the Karakoram range, on the border between Pakistan and China.\n",
      "User: And how tall is it?\n",
      "Assistant: K2 is 8,611 meters (28,251 feet) tall above sea level.\n",
      "\n",
      "--- Full Conversation History ---\n",
      "User: What's the highest mountain in the world?\n",
      "User: What's the highest mountain in the world?\n",
      "Assistant: The highest mountain in the world is Mount Everest, which stands at 8,848.86 meters (29,031.7 feet) above sea level. It is located in the Himalayas, on the border between Nepal and the Tibet Autonomous Region of China.\n",
      "User: What is the second?\n",
      "Assistant: The second highest mountain in the world is K2, also known as Mount Godwin-Austen. It stands at 8,611 meters (28,251 feet) above sea level and is located in the Karakoram range, on the border between Pakistan and China.\n",
      "User: And how tall is it?\n",
      "Assistant: K2 is 8,611 meters (28,251 feet) tall above sea level.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# You can continue adding more rounds\n",
    "user_message_3 = {\"role\": \"user\", \"content\": \"And how tall is it?\"}\n",
    "conversation_history.append(user_message_3)\n",
    "\n",
    "print(f\"User: {user_message_3['content']}\")\n",
    "assistant_response_3 = get_chat_completion(conversation_history)\n",
    "print(f\"Assistant: {assistant_response_3}\")\n",
    "\n",
    "# Add the assistant's response to the conversation history\n",
    "conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response_3})\n",
    "\n",
    "print(\"\\n--- Full Conversation History ---\")\n",
    "for message in conversation_history:\n",
    "    print(f\"{message['role'].capitalize()}: {message['content']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
